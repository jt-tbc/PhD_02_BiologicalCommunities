---
title: "C2_GAM_Script"
author: "Joe Turner"
date: "10 July 2017"
output: pdf_document
---

```{r setup, include=FALSE}

setwd("C:/OneDrive/C2_Coms")
GAMdata <- read.csv("31_AllData_GIS_Export_NoBlanks.csv", header = T, row.names = 1)
set.seed(333)

library(mgcv)
library(gamm4)
library(tidyr)
library(dplyr)
options(dplyr.width = Inf) #enables head() to display all coloums
library(mgcv)
library(MuMIn)
library(car)
library(doBy)
library(gplots)
library(ggplot2)
library(RColorBrewer)
library(doParallel)
library(gamm4)
library(zoo) #for filling in missing values



pred.vars <- colnames(GAMdata[,c(6:7, 12:17, 25:27, 44:76)])

# make so columns aren't negative
GAMdata[,c(6:7, 12:15, 25:27, 44:76)] <- lapply(GAMdata[,c(6:7, 12:15, 25:27, 44:76)], abs)


# Correalation of predictor variables----
correl <- round(cor(GAMdata[,pred.vars]),2)

# Write csv to examine correlation in detail
write.csv(correl, "32_Correlation_of_predictor_variables_for_GAM.csv", row.names = T)


# Check distributions----
pdf("Predictor_variable_distributions.pdf", title = "Predictor variable distributions")
par(mfrow=c(3,2))

for (i in pred.vars) {
   x<-GAMdata[ ,i]
   x = as.numeric(unlist(x))
   hist((x))#Looks best
   plot((x),main = paste(i))
   hist(sqrt(x))
   plot(sqrt(x))
   hist(log(x+1))
   plot(log(x+1))
 }

dev.off()

# Consult pdf and guess best transformations to be:

#"Long" - none
# "Lat" - none   
# "PSAL" - log(x+1)
# "TEMP" - log(x+1)       
# "CDOM" - none      
# "CPHL"  - none     
# "Oxygen" - remove    
# "Saturation" - remove
# "curv"  - none    
# "curv_plan" - none   
# "curv_prof"  - none  
# "east"  - none     
# "hyp3"  - none       
# "hyp5"  - none       
# "hyp10" - none       
# "hyp25" - none       
# "mean3"  - none    
# "mean5"  - none    
# "mean10" - none     
# "mean25"  - none    
# "north"  - none     
# "range3" - sqrt     
# "range5"  - sqrt    
# "range10" - sqrt   
# "range25" - sqrt   
# "rugosity"  - none 
# "slope" - sqrt    
# "std3" - sqrt      
# "std5"  - sqrt     
# "std10"  - sqrt    
#  std25"   - sqrt   
# "BPI_F_10"  - none  
# "BPI_F_15"  - none  
# "BPI_F_25"  - none  
# "BPI_B_50"  - none  
# "BPI_B_100" - none 
# "BPI_B_250" - none  
# BScatter"  - none  
# "Depth"   - none    
# "BroadDepth" - none 
# SST - log(x+1)
# Sand - log(x+1)
# Rock - log(x+1)
# Pebble/Gravel - log(x+1)

# change zero to NA
GAMdata$Oxygen[GAMdata$Oxygen == 0] <- NA
GAMdata$Saturation[GAMdata$Saturation == 0] <- NA
GAMdata$CDOM[GAMdata$CDOM == 0] <- NA
GAMdata$CPHL[GAMdata$CPHL == 0] <- NA

GAMdata<- GAMdata%>%
  mutate(PSAL.log=log(PSAL+1))%>%
  mutate(TEMP.log=log(TEMP+1))%>%
  mutate(SST.log=log(SST+1))%>%
  mutate(Rock.log=log(Rock+1))%>%
  mutate(PebbleGravel.log=log(PebbleGravel+1))%>%
  mutate(Sand.log=log(Sand+1))%>%
  # mutate(curv.sqrt=sqrt(curv))%>%
  # mutate(curv_plan.sqrt=sqrt(curv_plan))%>%
  # mutate(curv_prof.sqrt=sqrt(curv_prof))%>%
  # mutate(hyp3.sqrt=sqrt(hyp3))%>%
  # mutate(hyp5.sqrt=sqrt(hyp5))%>%
  # mutate(hyp10.sqrt=sqrt(hyp10))%>%
  # mutate(hyp25.sqrt=sqrt(hyp25))%>%
  mutate(range3.sqrt=sqrt(range3))%>%
  mutate(range5.sqrt=sqrt(range5))%>%
  mutate(range10.sqrt=sqrt(range10))%>%
  mutate(range25.sqrt=sqrt(range25))%>%
  mutate(std3.sqrt=sqrt(std3))%>%
  mutate(std5.sqrt=sqrt(std5))%>%
  mutate(std10.sqrt=sqrt(std10))%>%
  mutate(std25.sqrt=sqrt(std25))%>%
  mutate(slope.sqrt=sqrt(slope))


# change substrate layers to factors
GAMdata$L1Hab <- as.factor(GAMdata$L1Hab)
GAMdata$L2Hab <- as.factor(GAMdata$L2Hab)
GAMdata$L3Hab <- as.factor(GAMdata$L3Hab)

# redo predictor variables
pred.vars <- c("Long" , "Lat" , "CDOM" , "CPHL" , "Oxygen" , "Saturation" , "east" , "mean3" , "mean5" ,
                  "mean10" , "mean25" , "north" , "rugosity" , "BPI_F_10" , "BPI_F_15" , "BPI_F_25" , "BPI_B_50" ,
                  "BPI_B_100" , "BPI_B_250" , "BScatter" , "Depth" , "PSAL.log" , "TEMP.log" , "SST.log" , 
                  "curv" , "curv_plan" , "curv_prof" , "hyp3" , "hyp5" , "hyp10" ,
                  "hyp25" , "range3.sqrt" , "range5.sqrt" , "range10.sqrt" , "range25.sqrt" , "std3.sqrt" ,
                  "std5.sqrt" , "std10.sqrt" , "std25.sqrt" , "slope.sqrt" , "L1Hab" , "L2Hab" , "L3Hab" , "Rock.log" ,
                  "Sand.log" , "PebbleGravel.log")


################
## HARD CORAL ##
################
CoralGAM <- gam(HardCoral ~ Long + Lat + s(CDOM) + s(CPHL) + s(Oxygen) + s(Saturation) + s(east) + s(mean3) + s(mean5) +
                  s(mean10) + s(mean25) + s(north) + s(rugosity) + BPI_F_10 + BPI_F_15 + BPI_F_25 + BPI_B_50 +
                  BPI_B_100 + BPI_B_250 + s(BScatter) + s(Depth) + s(PSAL.log) + s(TEMP.log) + s(SST.log) + 
                  s(curv) + s(curv_plan) + s(curv_prof) + s(hyp3) + s(hyp5) + s(hyp10) +
                  s(hyp25) + s(range3.sqrt) + s(range5.sqrt) + s(range10.sqrt) + s(range25.sqrt) + s(std3.sqrt) +
                  s(std5.sqrt) + s(std10.sqrt) + s(std25.sqrt) + s(slope.sqrt) + L1Hab + L2Hab + L3Hab + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                select = T,
                na.action = na.exclude)

summary(CoralGAM)
anova(CoralGAM)
# remove non-significant variables
CoralGAM <- gam(HardCoral ~ s(east) + BPI_B_250 + s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)


CoralGAM <- gam(HardCoral ~ s(north) + s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)


summary(CoralGAM)
AIC(CoralGAM)
BIC(CoralGAM)

gam.check(CoralGAM, k.rep = 1000) 
plot(fitted(CoralGAM),residuals(CoralGAM))
plot(GAMdata$HardCoral,residuals(CoralGAM))
plot(CoralGAM, pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col="gray80")


# GAMdata$resid <- residuals(CoralGAM)
# library(reshape2)
# plotDF <- melt(GAMdata[, c("Long", "east","north","BPI_B_250","Sand.log", "PebbleGravel.log", "resid")], id="resid")
# library(ggplot2)
# ggplot(plotDF, aes(x=value, y=resid)) + 
#   geom_point() + facet_wrap(~ variable)


#######################
## BROWN MACROALGAE ##
######################

BrownMacroGAM <- gam(BrownMacroalgae ~ s(CDOM) + s(CPHL) + s(Oxygen) + s(Saturation) + s(east) + s(mean3) +
                       s(mean5) + s(mean10) + s(mean25) + s(north) + s(rugosity) + BPI_F_10 + BPI_F_15 + BPI_F_25 + BPI_B_50 +
                  BPI_B_100 + BPI_B_250 + s(BScatter) + s(Depth) + s(PSAL.log) + s(TEMP.log) + s(SST.log) + 
                  s(curv) + s(curv_plan) + s(curv_prof) + s(hyp3) + s(hyp5) + s(hyp10) +
                  s(hyp25) + s(range3.sqrt) + s(range5.sqrt) + s(range10.sqrt) + s(range25.sqrt) + s(std3.sqrt) +
                  s(std5.sqrt) + s(std10.sqrt) + s(std25.sqrt) + s(slope.sqrt) + L1Hab + L2Hab + L3Hab + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)
summary(BrownMacroGAM)
plot(BrownMacroGAM)
anova(BrownMacroGAM)

# remove non-significant variables
BrownMacroGAM <- gam(BrownMacroalgae ~ s(CPHL) + s(mean3) + s(TEMP.log) + s(range25.sqrt) +
                      s(std25.sqrt) + s(Rock.log) + s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

BrownMacroGAM <- gam(BrownMacroalgae ~ s(CPHL) + s(Depth) + s(TEMP.log) +
                       s(range25.sqrt) + s(Rock.log) + s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

summary(BrownMacroGAM)
AIC(BrownMacroGAM)
BIC(BrownMacroGAM)
gam.check(BrownMacroGAM, k.rep = 1000) 
plot(BrownMacroGAM, pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col="gray80")


#########
## CCA ##
#########

CCAGAM <- gam(CCA ~ s(CDOM) + s(CPHL) + s(Oxygen) + s(Saturation) + s(east) + s(mean3) + s(mean5) +
                  s(mean10) + s(mean25) + s(north) + s(rugosity) + BPI_F_10 + BPI_F_15 + BPI_F_25 + BPI_B_50 +
                  BPI_B_100 + BPI_B_250 + s(BScatter) + s(Depth) + s(PSAL.log) + s(TEMP.log) + s(SST.log) + 
                  s(curv) + s(curv_plan) + s(curv_prof) + s(hyp3) + s(hyp5) + s(hyp10) +
                  s(hyp25) + s(range3.sqrt) + s(range5.sqrt) + s(range10.sqrt) + s(range25.sqrt) + s(std3.sqrt) +
                  s(std5.sqrt) + s(std10.sqrt) + s(std25.sqrt) + s(slope.sqrt) + L1Hab + L2Hab + L3Hab + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)
summary(CCAGAM)
plot(CCAGAM)
anova(CCAGAM)
AIC(CCAGAM)

CCAGAM <- gam(CCA ~ s(TEMP.log) + s(SST.log) + s(range25.sqrt) + L2Hab + s(Rock.log) + s(Sand.log) +
                s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

CCAGAM <- gam(CCA ~ s(CPHL) + s(TEMP.log) + L2Hab + s(Rock.log) + s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)
summary(CCAGAM)
AIC(CCAGAM)
BIC(CCAGAM)
gam.check(CCAGAM, k.rep = 1000) 
plot(CCAGAM, pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col="gray80")



############
## SPONGE ##
############

SpongeGAM <- gam(Sponges ~ Long + Lat + s(CDOM) + s(CPHL) + s(Oxygen) + s(Saturation) + s(east) + s(mean3) + s(mean5) +
                  s(mean10) + s(mean25) + s(north) + s(rugosity) + BPI_F_10 + BPI_F_15 + BPI_F_25 + BPI_B_50 +
                  BPI_B_100 + BPI_B_250 + s(BScatter) + s(Depth) + s(PSAL.log) + s(TEMP.log) + s(SST.log) + 
                  s(curv) + s(curv_plan) + s(curv_prof) + s(hyp3) + s(hyp5) + s(hyp10) +
                  s(hyp25) + s(range3.sqrt) + s(range5.sqrt) + s(range10.sqrt) + s(range25.sqrt) + s(std3.sqrt) +
                  s(std5.sqrt) + s(std10.sqrt) + s(std25.sqrt) + s(slope.sqrt) + L1Hab + L2Hab + L3Hab + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)
summary(SpongeGAM)
plot(SpongeGAM)
anova(SpongeGAM)
AIC(SpongeGAM)

SpongeGAM <- gam(Sponges ~ s(mean25) + BPI_B_250 + s(std3.sqrt) + s(slope.sqrt) + L3Hab + s(Rock.log) + s(Sand.log) +
                   s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

SpongeGAM <- gam(Sponges ~ s(mean25) + BPI_B_250 + s(SST.log) + L2Hab + L3Hab + s(Rock.log) + s(Sand.log) +
                   s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

summary(SpongeGAM)
AIC(SpongeGAM)
BIC(SpongeGAM)
gam.check(SpongeGAM, k.rep = 1000) 
plot(SpongeGAM, pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col="gray80")



############
## NPFF ##
############

NPFFGAM <- gam(NPFF ~ Long + Lat + s(CDOM) + s(CPHL) + s(Oxygen) + s(Saturation) + s(east) + s(mean3) + s(mean5) +
                  s(mean10) + s(mean25) + s(north) + s(rugosity) + BPI_F_10 + BPI_F_15 + BPI_F_25 + BPI_B_50 +
                  BPI_B_100 + BPI_B_250 + s(BScatter) + s(Depth) + s(PSAL.log) + s(TEMP.log) + s(SST.log) + 
                  s(curv) + s(curv_plan) + s(curv_prof) + s(hyp3) + s(hyp5) + s(hyp10) +
                  s(hyp25) + s(range3.sqrt) + s(range5.sqrt) + s(range10.sqrt) + s(range25.sqrt) + s(std3.sqrt) +
                  s(std5.sqrt) + s(std10.sqrt) + s(std25.sqrt) + s(slope.sqrt) + L1Hab + L2Hab + L3Hab + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)
summary(NPFFGAM)
plot(NPFFGAM)
anova(NPFFGAM)
AIC(NPFFGAM)

NPFFGAM <- gam(NPFF ~ s(CDOM) + s(Saturation) + BPI_F_25 + s(range10.sqrt) + s(Rock.log) +
                  s(Sand.log) + s(PebbleGravel.log),
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

NPFFGAM <- gam(NPFF ~ s(CDOM) + s(range10.sqrt) + s(Rock.log) + s(Sand.log) + s(PebbleGravel.log) + L2Hab,
                family = tw(),
                data = GAMdata,
                na.action = na.exclude)

summary(NPFFGAM)
AIC(NPFFGAM)
BIC(NPFFGAM)
gam.check(NPFFGAM, k.rep = 1000) 
plot(NPFFGAM, pages=1,residuals=TRUE,all.terms=TRUE,shade=TRUE,shade.col="gray80")



```

```{r GAM FULL SUBSET MODEL FUNCTION}

full.subsets.gam=function(use.dat,
                          test.fit,
                          pred.vars.cont=NA,
                          pred.vars.fact=NA,
                          cyclic.vars=NA,
                          linear.vars=NA,
                          smooth.interactions=pred.vars.fact,
                          factor.interactions=F,
                          cov.cutoff=0.28,
                          size=3,
                          k=5,
                          bs.arg="'cr'",
                          null.terms="",
                          max.models=500,
                          parallel=F,
                          n.cores=4,
                          r2.type="r2.lm.est",
                          report.unique.r2=F){

  # make an "intercept" term for the null model
  use.dat$intercept=1
  interaction.terms=NA
  linear.interaction.terms=NA
  all.predictors=na.omit(c(pred.vars.cont,pred.vars.fact,linear.vars))
  included.vars=all.predictors

  # check the null model will fit
  if(nchar(null.terms)>0){
    null.formula=as.formula(paste("~ intercept-1",null.terms,sep="+"))}else{
    null.formula=as.formula("~ intercept-1")}
  null.fit=try(update(test.fit,formula=null.formula,
                data=use.dat),silent=T)
  if(class(null.fit)[1]=="try-error"){
        stop(paste("Null model not successfully fitted, please check your inputs.
                   If there are no random effects try using 'gam' instead of 'uGamm' 
                   in your test.fit model call.",
                   " ",
                   "The following error message was provided:  ",
                   "  ",
                   null.fit, ""))}
  # check for missing predictor values
  if(max(is.na(use.dat[,all.predictors]))==1){
        stop("Predictor variables contain NA and AICc/BIC comparisons are invalid. 
        Remove rows with NA from the input data or interpolate missing predictors.")}

  # if there are factors
  if(length(na.omit(pred.vars.fact))>0){
  # if there are more than two factors
  if(length(na.omit(pred.vars.fact))>1){
    # make all the interactions between factors
    if(factor.interactions==T){
        if(length(pred.vars.fact)<2){
            stop("You have less than 2 factors. Please reset 'factor.interactions' to 'False'")}
      factor.correlations=check.correlations(use.dat[,pred.vars.fact])
      fact.combns=list()
      fact.cmbns.size=size
      if(size>length(pred.vars.fact)){fact.cmbns.size=length(pred.vars.fact)}
      for(i in 2:fact.cmbns.size){
        if(i<=length(pred.vars.fact)){
        fact.combns=c(fact.combns,
         combn(pred.vars.fact,i,simplify=F)) }}
        # check which were correlated
        fact.combns=lapply(fact.combns,FUN=function(x){
                row.index=which(match(rownames(factor.correlations),x)>0)
                col.index=which(match(colnames(factor.correlations),x)>0)
                cor.mat.m=factor.correlations[row.index,col.index]
                out=x
                if(max(abs(cor.mat.m[upper.tri(cor.mat.m)]))>cov.cutoff){out=NA}
                return(out)})
        fact.combns[which(is.na(fact.combns))]=NULL
        tt=data.frame(lapply(fact.combns,FUN=function(x){
                   do.call("paste",as.list(use.dat[,x]))}))
        factor.interaction.terms=unlist(lapply(fact.combns,FUN=paste,collapse=".I."))
        colnames(tt)=factor.interaction.terms

      use.dat=cbind(use.dat,tt)
      pred.vars.fact=c(pred.vars.fact,factor.interaction.terms)
    }
   }
   # make sure the factors are factors
   for(f in 1:length(pred.vars.fact)){
       use.dat[,pred.vars.fact[f]]=factor(use.dat[,pred.vars.fact[f]])}

   # check which ones should be included as interactions with the smoothers
   smooth.interactions=pred.vars.fact[which(unlist(lapply(strsplit(pred.vars.fact,
      split=".I."),function(x){
      max(is.na(match(x,smooth.interactions)))}))==0)]

   # make the interaction terms between the factors and continuous predictors
   if(length(na.omit(smooth.interactions))>0){
    all.interactions=expand.grid(setdiff(pred.vars.cont,linear.vars),smooth.interactions)
    interaction.terms=paste(all.interactions$Var1,all.interactions$Var2,sep=".by.")

    # now interactions between continous predictors and factors
    if(length(na.omit(linear.vars))>0){
     linear.interactions=expand.grid(linear.vars,smooth.interactions)
     linear.interaction.terms=paste(linear.interactions$Var1,linear.interactions$Var2,
                                sep=".t.")}
   }}

  all.predictors=na.omit(unique(c(all.predictors,pred.vars.fact)))
  # calculate a correlation matrix between all predictors
  cor.matrix=check.correlations(use.dat[,all.predictors],parallel=parallel,n.cores=n.cores)
  # replace NA's with zero.
  cor.matrix[which(cor.matrix=="NaN")]=0

  # make all possible combinations
  if(length(na.omit(c(pred.vars.cont,
                      pred.vars.fact)))<size){
        stop("Model size is greater than the number of predictors.")}
  all.mods=list()
  for(i in 1:size){
    all.mods=c(all.mods,
     combn(na.omit(c(pred.vars.cont,pred.vars.fact,
                     interaction.terms,linear.interaction.terms)),
                     i,simplify=F))
  }

  # remove redundant models
  use.mods=all.mods
  for(m in 1:length(all.mods)){
    mod.m=all.mods[[m]]
    mod.terms=unlist(strsplit(unlist(strsplit(mod.m,split=".by.",fixed=T)),
                               split=".t.",fixed=T))
    n.vars.m=unique(unlist(strsplit(unlist(strsplit(unlist(strsplit(mod.m,split=".by.",fixed=T)),
                                  split=".I.",fixed=T)), split=".t.",fixed=T)))
    cont.vars=na.omit(na.omit(c(pred.vars.cont,linear.vars))[match(mod.terms,na.omit(c(pred.vars.cont,linear.vars)))])
    fact.vars=unique(na.omit(pred.vars.fact[match(mod.terms,pred.vars.fact)]))

    # if there are factor vars
    if(length(fact.vars)>0){
      # check that any "by" factor vars are accompanied by a + term in its owns right
      if(max(is.na(match(fact.vars,mod.m)))==1){use.mods[[m]]=NA}}

    # remove the model if the predictors are correlated
    if(length(mod.terms)>1){
     row.index=which(match(rownames(cor.matrix),unique(mod.terms))>0)
     col.index=which(match(colnames(cor.matrix),unique(mod.terms))>0)
     cor.mat.m=cor.matrix[row.index,col.index]
     if(max(abs(cor.mat.m[upper.tri(cor.mat.m)]))>cov.cutoff){use.mods[[m]]=NA}
    }

    # remove the model if there are more than the number of terms specified in "size"
    if(length(n.vars.m)>size){use.mods[[m]]=NA}

    # remove the models if a continuous predictor occurs as a by and as a single term
    if(length(cont.vars)>length(unique(cont.vars))){use.mods[[m]]=NA}
  }

  use.mods[which(is.na(use.mods))]=NULL

  # now make the models into gamm formula
  if(nchar(null.terms)==0){# if there is no bs='re' random effect random effect
                             # or other null term in the null model
    mod.formula=list(as.formula("~ intercept-1"))}
  if(nchar(null.terms)>0){# to add a bs='re' random effect
    mod.formula=list(null.formula)}

  for(m in 1:length(use.mods)){
     mod.m=use.mods[[m]]
     cont.smooths=mod.m[which(match(mod.m,setdiff(pred.vars.cont,linear.vars))>0)]
     by.smooths=mod.m[grep(".by.",mod.m)]
     factor.terms=mod.m[which(match(mod.m,pred.vars.fact)>0)]
     linear.terms=mod.m[which(match(mod.m,linear.vars)>0)]
     linear.interaction.terms=mod.m[grep(paste(linear.vars,".t.",sep=""),mod.m)]
     all.terms.vec=character()

     if(length(cont.smooths>0)){all.terms.vec=c(all.terms.vec,
                  paste("s(",cont.smooths,",k=",k,",bs=",bs.arg,")",sep=""))}
     if(length(by.smooths>0)){all.terms.vec=c(all.terms.vec,
         paste("s(",gsub(".by.",",by=",by.smooths),",k=",k,",bs=",bs.arg,")",sep=""))}
     if(length(linear.interaction.terms>0)){all.terms.vec=c(all.terms.vec,
               gsub(".t.","*",linear.interaction.terms,fixed=T))}
     if(length(factor.terms>0)){all.terms.vec=c(all.terms.vec,factor.terms)}
     if(max(is.na(cyclic.vars))!=1){
       for(cc in 1:length(cyclic.vars)){
           for(v in 1:length(all.terms.vec)){
             if(length(grep(cyclic.vars[cc],all.terms.vec[v]))>0){
                  all.terms.vec[v]=gsub(paste("bs=",bs.arg,sep=""),"bs='cc'",all.terms.vec[v])
                  }}}}
     if(nchar(null.terms)==0){# if there is no bs='re' random effect
                                # or other null term in the null model
       formula.m=as.formula(paste("~",
               paste(all.terms.vec,collapse="+")))}
     if(nchar(null.terms)>0){#
       formula.m=as.formula(paste("~",
               paste(c(all.terms.vec,null.terms),collapse="+")))}
     mod.formula=c(mod.formula,list(formula.m))
  }

  names(mod.formula)=c("null",lapply(use.mods,FUN=paste,collapse="+"))

  # Is this too many models?
  n.mods=length(mod.formula)
  time.to.run=round(system.time(try(update(test.fit,formula=mod.formula[[n.mods]],data=use.dat),silent=T))[3]*n.mods/60)
  test.mod=try(update(test.fit,formula=mod.formula[[n.mods]],data=use.dat),silent=T)
  mod.gbs=round(object.size(test.mod)/1073741824*n.mods,1)
  if(n.mods>max.models){
        stop(paste("You have ",n.mods," models. If you want to fit all of these you need to
        increase 'max.models' from ",max.models,". Otherwise, if the model set
        is larger than you can realistically fit, try reducing the number of predictors,
        setting the covariance 'cov.cutoff' argument to less than ", cov.cutoff,
        "
        or setting 'factor.interactions' to FALSE (if you have factors).
        To run the current model set will take approximately ",time.to.run," minutes
        and use ",mod.gbs," gigabytes of additional memory.",sep=""))
       }

  # now fit the models by updating the test fit (with or without parallel)
  if(parallel==T){
   require(doParallel)
   cl=makePSOCKcluster(n.cores)
   registerDoParallel(cl)
   out.dat<-foreach(l = 1:length(mod.formula),
                   .packages=c('mgcv','gamm4','MuMIn'),
                   .errorhandling='pass')%dopar%{
           out=update(test.fit,formula=mod.formula[[l]],data=use.dat)}
   stopCluster(cl)
   registerDoSEQ()
           }else{
      out.dat=list()
      for(l in 1:length(mod.formula)){
        out=update(test.fit,formula=mod.formula[[l]],data=use.dat)
        out.dat=c(out.dat,list(out))}
  }
  names(out.dat)=names(mod.formula[1:n.mods])

  # find all the models that didn't fit and extract the error messages
  model.success=lapply(lapply(out.dat,FUN=class),FUN=function(x){
     x[1]=="gamm" | x[1]=="gamm4" | x[1]=="gam"})
  failed.models=mod.formula[which(model.success==F)]
  success.models=out.dat[which(model.success==T)]
  if(length(success.models)==0){
        stop("None of your models fitted successfully. Please check your input objects.")}

  # some functions for extracting model information
  require(MuMIn)
  wi<<-function(AIC.vals){# This function calculate the Aikaike weights:
   # wi=(exp(-1/2*AICc.vals.adj))/Sum.wi=1 to r (exp(-1/2*AICc.vals.adj))
   AICc.vals.adj=AIC.vals-min(na.omit(AIC.vals))
   wi.den=rep(NA,length(AICc.vals.adj))
   for(i in 1:length(AICc.vals.adj)){
    wi.den[i]=exp(-1/2*AICc.vals.adj[i])}
   wi.den.sum=sum(na.omit(wi.den))
   wi=wi.den/wi.den.sum
   return(wi)}

  # of the successful models, make a table indicating which variables are included
  var.inclusions=matrix(0,ncol=length(included.vars),length(success.models))
  colnames(var.inclusions)=c(included.vars)

  for(m in 1:length(success.models)){
        pred.vars.m=unique(
          unlist(strsplit(unlist(strsplit(unlist(strsplit(unlist(strsplit(unlist(strsplit(names(success.models)[m],
          split="+",fixed=T)),split=".by.",fixed=T)),split=".I.",fixed=T)),
          split="*",fixed=T)),split=".t.",fixed=T)))
        if(pred.vars.m[1]!="null"){var.inclusions[m,pred.vars.m]=1}}

  # now make a table of all the model summary data
  mod.data.out=data.frame("modname"=names(success.models))
  mod.data.out$AICc=unlist(lapply(success.models,FUN=AICc))
  mod.data.out$BIC=unlist(lapply(success.models,FUN=BIC))
  mod.data.out$delta.AICc=round(mod.data.out$AICc-min(mod.data.out$AICc),3)
  mod.data.out$delta.BIC=round(mod.data.out$BIC-min(mod.data.out$BIC),3)
  mod.data.out$wi.AICc=round(wi(mod.data.out$AICc),3)
  mod.data.out$wi.BIC=round(wi(mod.data.out$BIC),3)
  mod.data.out$r2.vals=round(unlist(lapply(success.models,FUN=function(x){
        if(class(x)[1]=="gam" & r2.type=="dev"){out=summary(x)$dev.expl}
        if(class(x)[1]=="gam" & r2.type=="r2"){out=summary(x)$r.sq}
        if(class(x)[1]=="gam" & r2.type=="r2.lm.est"){
           out=summary(lm(x$y~predict(x)))$r.sq}
        if(class(x)[[1]]=="gamm4" & r2.type=="dev"){
           out=summary(x$gam)$dev.expl
           if(length(out)==0){out=NA}}
        if(class(x)[[1]]=="gamm4" & r2.type=="r2"){out=summary(x$gam)$r.sq}
        if(class(x)[[1]]=="gamm4" & r2.type=="r2.lm.est"){
           out=summary(lm(attributes(x$mer)$frame$y~predict(x,re.form=NA)))$r.sq}
        return(out)})),3)

  # substract the null model r2 value from each model r2 value
  if(report.unique.r2==T){
  null.r2=mod.data.out$r2.vals[which(mod.data.out$modname=="null")]
  mod.data.out$r2.vals.unique=mod.data.out$r2.vals-null.r2}

  # now calculate the summed edf
  mod.data.out$edf=round(unlist(lapply(success.models,FUN=function(x){
        if(class(x)[1]=="gam"){
          edf.m=summary(x)$edf
          p.coeff.m=summary(x)$p.coeff}else{
           edf.m=summary(x$gam)$edf
           p.coeff.m=summary(x$gam)$p.coeff}
        edf.m[which(edf.m<1)]=1 # any edf<0 are reset to 1 to ensure proper
                                # parameter count when there is shrinkage (bs='cc')
        return(sum(c(edf.m,length(p.coeff.m))))})),2)
  # count the edf values less than 0.25 to check for serious shrinkage
  mod.data.out$edf.less.1=unlist(lapply(success.models,FUN=function(x){
        if(class(x)[1]=="gam"){edf.m=summary(x)$edf}else{edf.m=summary(x$gam)$edf}
        return(length(which(edf.m<0.25)))}))
  # now add columns for the included predictors to the dataframe
  mod.data.out=cbind(mod.data.out,var.inclusions)

  # now calculate the variable importance
  # first for AICc
  variable.weights.raw=colSums(mod.data.out[,included.vars]*mod.data.out$wi.AICc)
  aic.var.weights=list(variable.weights.raw=variable.weights.raw)
  # next for BIC
  variable.weights.raw=colSums(mod.data.out[,included.vars]*mod.data.out$wi.BIC)
  bic.var.weights=list(variable.weights.raw=variable.weights.raw)
  # now return the list of outputs
  return(list(mod.data.out=mod.data.out,
              used.data=use.dat,
              predictor.correlations=cor.matrix,
              failed.models=failed.models,
              success.models=success.models,
              variable.importance=
                 list(aic=aic.var.weights,bic=bic.var.weights)))
}
#------------------ end function --------------------------------------------#

```

```{r function: CHECK CORRELATIONS}

check.correlations=function(dat,parallel=F,n.cores=4){
  classes.dat=sapply(dat,class)
  fact.vars=names(which(classes.dat=="factor" | classes.dat=="character"))
  cont.vars=names(which(classes.dat=="integer" | classes.dat=="numeric"))
  if(length(cont.vars)>1){
   cor.mat=cor(dat[,cont.vars],use="pairwise.complete.obs")}else{
   cor.mat=matrix(1,ncol=1,nrow=1)
   colnames(cor.mat)=cont.vars
   rownames(cor.mat)=cont.vars}
  if(length(fact.vars)>0){
   if(length(cont.vars)>0){
    lm.grid=expand.grid(list(fact.var=fact.vars,cont.var=cont.vars))
    r.estimates=cbind(lm.grid,apply(lm.grid,MARGIN=1,FUN=function(x){
        sqrt(summary(lm(dat[,x[2]]~factor(dat[,x[1]])))$r.sq)}))

    fact.cont.upper.right=matrix(NA,ncol=length(fact.vars),nrow=length(cont.vars))
    colnames(fact.cont.upper.right)=fact.vars;rownames(fact.cont.upper.right)=cont.vars

    fact.cont.lower.left=matrix(NA,ncol=length(cont.vars),nrow=length(fact.vars))
    colnames(fact.cont.lower.left)=cont.vars;rownames(fact.cont.lower.left)=fact.vars

    fact.fact.lower.right=matrix(NA,ncol=length(fact.vars),nrow=length(fact.vars))
    colnames(fact.fact.lower.right)=fact.vars;rownames(fact.fact.lower.right)=fact.vars

    out.cor.mat=rbind(cbind(cor.mat,fact.cont.upper.right),
                      cbind(fact.cont.lower.left,fact.fact.lower.right))

    # assign the estimated r values to the upper right and lower left corners
    for(r in 1:nrow(r.estimates)){
       # upper right
       col.index=which(colnames(out.cor.mat)==r.estimates$fact.var[r])
       row.index=which(rownames(out.cor.mat)==r.estimates$cont.var[r])
       out.cor.mat[row.index,col.index]=r.estimates[r,3]
       # lower left
       col.index=which(colnames(out.cor.mat)==r.estimates$cont.var[r])
       row.index=which(rownames(out.cor.mat)==r.estimates$fact.var[r])
       out.cor.mat[row.index,col.index]=r.estimates[r,3]
     }
    }else{
        fact.fact.lower.right=matrix(NA,ncol=length(fact.vars),nrow=length(fact.vars))
    colnames(fact.fact.lower.right)=fact.vars;rownames(fact.fact.lower.right)=fact.vars
    out.cor.mat=fact.fact.lower.right}

  # estimate r values for fact-fact combinations
  lm.grid=expand.grid(list(fact.var1=fact.vars,fact.var2=fact.vars))
  require(nnet)
  if(parallel==T){
   require(doParallel)
   cl=makePSOCKcluster(n.cores)
   registerDoParallel(cl)
   out.cor.dat<-foreach(r = 1:nrow(lm.grid),.packages=c('nnet'),.errorhandling='pass')%dopar%{
    var.1=as.character(lm.grid[r,1])
    var.2=as.character(lm.grid[r,2])
    dat.r=na.omit(dat[,c(var.1,var.2)])
    fit <- try(summary(multinom(dat.r[,var.1] ~ dat.r[,var.2],trace=F))$deviance,silent=T)
    null.fit=try(summary(multinom(dat[,var.1] ~ 1,trace=F))$deviance,silent=T)
    if(class(fit)!="try-error"){
      r.est=sqrt(1-(fit/null.fit))
      c(var.1,var.2,r.est)}}
   stopCluster(cl)
   registerDoSEQ()}else{
    out.cor.dat=list()
    for(r in 1:nrow(lm.grid)){
          var.1=as.character(lm.grid[r,1])
          var.2=as.character(lm.grid[r,2])
          dat.r=na.omit(dat[,c(var.1,var.2)])
          fit <- try(summary(multinom(dat.r[,var.1] ~ dat.r[,var.2],trace=F))$deviance,silent=T)
          null.fit=try(summary(multinom(dat[,var.1] ~ 1,trace=F))$deviance,silent=T)
          out=NA
          if(class(fit)!="try-error"){
                   r.est=sqrt(1-(fit/null.fit))
                   out=c(var.1,var.2,r.est)}
      out.cor.dat=c(out.cor.dat,list(out))}
      }

    for(r in 1:length(out.cor.dat)){
       out.cor.mat[which(colnames(out.cor.mat)==out.cor.dat[[r]][1]),
                   which(rownames(out.cor.mat)==out.cor.dat[[r]][2])]=
                   as.numeric(out.cor.dat[[r]][3])}}else{out.cor.mat=cor.mat}
  return(out.cor.mat)
}

```

```{r BECKY MODEL CODE}

setwd("C:/OneDrive/C2_Coms")
GAMdata <- read.csv("31_AllData_GIS_Export_NoBlanks.csv", header = T, row.names = 1)
set.seed(333)

library(mgcv)
library(gamm4)
library(tidyr)
library(dplyr)
options(dplyr.width = Inf) #enables head() to display all coloums
library(mgcv)
library(MuMIn)
library(car)
library(doBy)
library(gplots)
library(ggplot2)
library(RColorBrewer)
library(doParallel)
library(gamm4)
library(zoo) #for filling in missing values


# change zero to NA
GAMdata$Oxygen[GAMdata$Oxygen == 0] <- NA
GAMdata$Saturation[GAMdata$Saturation == 0] <- NA
GAMdata$CDOM[GAMdata$CDOM == 0] <- NA
GAMdata$CPHL[GAMdata$CPHL == 0] <- NA

GAMdata<- GAMdata%>%
  mutate(PSAL.log=log(PSAL+1))%>%
  mutate(TEMP.log=log(TEMP+1))%>%
  mutate(SST.log=log(SST+1))%>%
  mutate(Rock.log=log(Rock+1))%>%
  mutate(PebbleGravel.log=log(PebbleGravel+1))%>%
  mutate(Sand.log=log(Sand+1))%>%
  # mutate(curv.sqrt=sqrt(curv))%>%
  # mutate(curv_plan.sqrt=sqrt(curv_plan))%>%
  # mutate(curv_prof.sqrt=sqrt(curv_prof))%>%
  # mutate(hyp3.sqrt=sqrt(hyp3))%>%
  # mutate(hyp5.sqrt=sqrt(hyp5))%>%
  # mutate(hyp10.sqrt=sqrt(hyp10))%>%
  # mutate(hyp25.sqrt=sqrt(hyp25))%>%
  mutate(range3.sqrt=sqrt(range3))%>%
  mutate(range5.sqrt=sqrt(range5))%>%
  mutate(range10.sqrt=sqrt(range10))%>%
  mutate(range25.sqrt=sqrt(range25))%>%
  mutate(std3.sqrt=sqrt(std3))%>%
  mutate(std5.sqrt=sqrt(std5))%>%
  mutate(std10.sqrt=sqrt(std10))%>%
  mutate(std25.sqrt=sqrt(std25))%>%
  mutate(slope.sqrt=sqrt(slope))


# change substrate layers to factors
GAMdata$L1Hab <- as.factor(GAMdata$L1Hab)
GAMdata$L2Hab <- as.factor(GAMdata$L2Hab)
GAMdata$L3Hab <- as.factor(GAMdata$L3Hab)

# redo predictor variables
pred.vars <- c("Long" , "Lat" , "CDOM" , "CPHL" , "Oxygen" , "Saturation" , "east" , "mean3" , "mean5" ,
                  "mean10" , "mean25" , "north" , "rugosity" , "BPI_F_10" , "BPI_F_15" , "BPI_F_25" , "BPI_B_50" ,
                  "BPI_B_100" , "BPI_B_250" , "BScatter" , "Depth" , "PSAL.log" , "TEMP.log" , "SST.log" , 
                  "curv" , "curv_plan" , "curv_prof" , "hyp3" , "hyp5" , "hyp10" ,
                  "hyp25" , "range3.sqrt" , "range5.sqrt" , "range10.sqrt" , "range25.sqrt" , "std3.sqrt" ,
                  "std5.sqrt" , "std10.sqrt" , "std25.sqrt" , "slope.sqrt" , "L1Hab" , "L2Hab" , "L3Hab" , "Rock.log" ,
                  "Sand.log" , "PebbleGravel.log")



# Source functions----
library(RCurl)
function_full_subsets_gam <- getURL("https://raw.githubusercontent.com/beckyfisher/FSSgam/master/function_full_subsets_gam_v1.11.R?token=AOSO6tZYAozKTAZ1Kt-aqlQIsiKuxONjks5ZZCtiwA%3D%3D", ssl.verifypeer = FALSE)
eval(parse(text = function_full_subsets_gam))

function_check_correlations <- getURL("https://raw.githubusercontent.com/beckyfisher/FSSgam/master/function_check_correlations_v1.00.R?token=AOSO6uxF2ON3UFyXj10uqm_N_94ZSEM3ks5ZZCyCwA%3D%3D", ssl.verifypeer = FALSE)
eval(parse(text = function_check_correlations))


# Run the full subset model selection----
# Set directory for the model outputs-
setwd("C:/OneDrive/C2_Coms/GAMs/Results")

dat <- GAMdata[, c("HardCoral", "BrownMacroalgae", "NPFF", "Sponges", "CCA","Long" , "Lat" , "CDOM" , "CPHL" , "Oxygen" ,
                   "Saturation" , "east" , "mean3" , "mean5" ,
                  "mean10" , "mean25" , "north" , "rugosity" , "BPI_F_10" , "BPI_F_15" , "BPI_F_25" , "BPI_B_50" ,
                  "BPI_B_100" , "BPI_B_250" , "BScatter" , "Depth" , "PSAL.log" , "TEMP.log" , "SST.log" , 
                  "curv" , "curv_plan" , "curv_prof" , "hyp3" , "hyp5" , "hyp10" ,
                  "hyp25" , "range3.sqrt" , "range5.sqrt" , "range10.sqrt" , "range25.sqrt" , "std3.sqrt" ,
                  "std5.sqrt" , "std10.sqrt" , "std25.sqrt" , "slope.sqrt" , "L1Hab" , "L2Hab" , "L3Hab" , "Rock.log" ,
                  "Sand.log" , "PebbleGravel.log")]
use.dat=dat
out.all=list()
var.imp=list()

cat.preds= c("L1Hab" , "L2Hab" , "L3Hab")
null.vars=c("Long","Lat", "Sand.log") # use as random effect and null model
cont.preds=c("BPI_F_10", "BPI_F_15", "BPI_F_25", "BPI_B_50", "BPI_B_100", "BPI_B_250", "CDOM" , "CPHL" , "Oxygen" ,
             "Saturation" , "east" , "mean3" , "mean5" ,
                  "mean10" , "mean25" , "north" , "rugosity" , "BScatter" , "Depth" , "PSAL.log" , "TEMP.log" , "SST.log" , 
                  "curv" , "curv_plan" , "curv_prof" , "hyp3" , "hyp5" , "hyp10" ,
                  "hyp25" , "range3.sqrt" , "range5.sqrt" , "range10.sqrt" , "range25.sqrt" , "std3.sqrt" ,
                  "std5.sqrt" , "std10.sqrt" , "std25.sqrt" , "slope.sqrt" , "Rock.log" ,
                  "PebbleGravel.log") # use as continuous predictors.
cor(dat[,cont.preds])
# have a look at the distribution of the continuous predictors
pdf(file="pred_vars.pdf",onefile=T)
for(p in 1:length(cont.preds)){
par(mfrow=c(2,1))
 hist(dat[,cont.preds[p]],main=cont.preds[p])
 plot(jitter(dat[,cont.preds[p]]))
 }
dev.off()

resp.vars.fams=list("HardCoral"=tw()
                    # , "BrownMacroalgae"=tw(),
                    # "NPFF"=tw(),
                    # "Sponges"=tw(),
                    # "CCA"=tw()
                    )
resp.vars=names(resp.vars.fams)
# take a look at the response variables
pdf(file="resp_vars.pdf",onefile=T)
for(r in 1:length(resp.vars)){
par(mfrow=c(2,1))
 hist(dat[,resp.vars[r]],main=resp.vars[r])
 plot(jitter(dat[,resp.vars[r]]))
 }
dev.off()


### now fit the models ---------------------------------------------------------
i=1
out.all=list()
var.imp=list()
fss.all=list()
top.all=list()
pdf(file="mod_fits.pdf",onefile=T)
for(i in 1:length(resp.vars)){
 use.dat=na.omit(dat[,c(null.vars,cont.preds,cat.preds,resp.vars[i])])
 use.dat$response=use.dat[,resp.vars[i]]
 Model1=gam(response~s(Sand.log,k=4,bs='cr'),
                    family=resp.vars.fams[[i]],
                    data=use.dat)
 out.list=full.subsets.gam(use.dat= use.dat,size=5,   # limit size here because null model already complex
                           test.fit=Model1,k=3,
                           pred.vars.cont=cont.preds,
                           pred.vars.fact=cat.preds,
                           max.models = 10000,
                           null.terms="s(Sand.log,k=4,bs='cr')")
 #names(out.list)
 # examine the list of failed models
 #out.list$failed.models
 #out.list$success.models
 fss.all=c(fss.all,list(out.list))
 mod.table=out.list$mod.data.out
 mod.table=mod.table[order(mod.table$AICc),]
 out.i=mod.table
 out.all=c(out.all,list(out.i))
 var.imp=c(var.imp,list(out.list$variable.importance$aic$variable.weights.raw))
 all.less.2AICc=mod.table[which(mod.table$delta.AICc<2),]
 top.all=c(top.all,list(all.less.2AICc))

 # plot the all best models
 par(oma=c(1,1,4,1))
 for(r in 1:nrow(all.less.2AICc)){
 best.model.name=as.character(all.less.2AICc$modname[r])
 best.model=out.list$success.models[[best.model.name]]
 if(best.model.name!="null"){
  plot(best.model,all.terms=T,pages=1,residuals=T,pch=16)
  mtext(side=3,text=resp.vars[i],outer=T)}
 }
}
dev.off()

names(out.all)=resp.vars
names(var.imp)=resp.vars
names(top.all)=resp.vars
names(fss.all)=resp.vars

all.mod.fits=do.call("rbind",out.all)
all.var.imp=do.call("rbind",var.imp)
top.mod.fits=do.call("rbind",top.all)

require(car)
require(doBy)
require(gplots)
require(RColorBrewer)

pdf(file="var_importance_heatmap.pdf",height=5,width=7,pointsize=10)
heatmap.2(all.var.imp,notecex=0.4,  dendrogram ="none",
                     col=colorRampPalette(c("white","yellow","orange","red"))(30),
                     trace="none",key.title = "",keysize=2,
                     notecol="black",key=T,
                     sepcolor = "black",margins=c(12,14), lhei=c(3,10),lwid=c(3,10),
                     Rowv=FALSE,Colv=FALSE)
dev.off()

write.csv(all.mod.fits,"all_model_fits.csv")
write.csv(top.mod.fits,"top_model_fits.csv")
write.csv(out.list$predictor.correlations,"predictor_correlations.csv")

#### pretty plots of best models -----------------------------------------------
zones=levels(dat$ZONE)
pdf("best_top_model_quick_plots.pdf",height=8,width=7,pointsize=12)
par(mfcol=c(4,2),mar=c(4,4,0.5,0.5),oma=c(2,0.5,0.5,0.5),bty="l")
for(r in 1:length(resp.vars)){
      tab.r=out.all[[resp.vars[r]]]
      top.mods.r=tab.r[1,]
      mod.r.m=as.character(top.mods.r[1,"modname"])
      mod.m=fss.all[[resp.vars[r]]]$success.models[[mod.r.m]]
      mod.vars=unique(unlist(strsplit(unlist(strsplit(mod.r.m,split="+",fixed=T)),
                       split=".by.")))
      # which continuous predictor is the variable included?
      plot.var=as.character(na.omit(mod.vars[match(cont.preds,mod.vars)]))
      # plot that variables, with symbol colours for zone
      plot(dat[,plot.var],dat[,resp.vars[r]],pch=16,
         ylab=resp.vars[r],xlab=plot.var,col=dat$ZONE)
      legend("topleft",legend=paste("(",LETTERS[r],")",sep=""),
             bty="n")
      range.v=range(dat[,plot.var])
      seq.v=seq(range.v[1],range.v[2],length=20)
      newdat.list=list(seq.v,# across the range of the included variable
                       mean(use.dat$depth), # for a median depth
                       mean(use.dat$SQRTSA),# for a median SQRTSA
                       "MANGROVE", # pick the first site, except don't predict on
                               # this by setting terms=c(plot.var,"ZONE")
                       zones)  # for each zone
      names(newdat.list)=c(plot.var,"depth","SQRTSA","site","ZONE")
      pred.vals=predict(mod.m,newdata=expand.grid(newdat.list),
                     type="response",se=T,exclude=c("site","SQRTSA","depth"))
      for(z in 1:length(zones)){
       zone.index=which(expand.grid(newdat.list)$ZONE==zones[z])
       lines(seq.v,pred.vals$fit[zone.index],col=z)
       lines(seq.v,pred.vals$fit[zone.index]+pred.vals$se[zone.index]*1.96,lty=3,col=z)
       lines(seq.v,pred.vals$fit[zone.index]-pred.vals$se[zone.index]*1.96,lty=3,col=z)}
}
legend("bottom",legend= zones,bty="n",ncol=2,col=c(1,2),pch=c(16,16),
   inset=-0.61,xpd=NA,cex=.8)
dev.off()



```

```{r TIM PLOTS}

# Plotting defaults----

# Theme-
Theme1 <-
  theme( # use theme_get() to see available options
    panel.grid.major = element_blank(), 
    panel.grid.minor = element_blank(), 
    # legend.background = element_rect(fill="white"),
    legend.background = element_blank(),
    legend.key = element_blank(), # switch off the rectangle around symbols in the legend
    legend.text = element_text(size=15),
    legend.title = element_blank(),
    legend.position = c(0.2, 0.8),
    text=element_text(size=15),
    strip.text.y = element_text(size = 15,angle = 0),
    axis.title.x=element_text(vjust=0.3, size=15),
    axis.title.y=element_text(vjust=0.6, angle=90, size=15),
    axis.text.x=element_text(size=15),
    axis.text.y=element_text(size=15),
    axis.line.x=element_line(colour="black", size=0.5,linetype='solid'),
    axis.line.y=element_line(colour="black", size=0.5,linetype='solid'),
    strip.background = element_blank())

# # functions for summarising data on plots----
se <- function(x) sd(x) / sqrt(length(x))
se.min <- function(x) (mean(x)) - se(x)
se.max <- function(x) (mean(x)) + se(x)


```

```{r}

# https://www.r-bloggers.com/finding-the-best-subset-of-a-gam-using-tabu-search-and-visualizing-it-in-r/

library(mgcv)
library(tabuSearch)
# http://archive.ics.uci.edu/ml/datasets/Housing
housing <- GAMdata 
housing <- housing[, c("HardCoral", "Long" , "Lat" , "CDOM" , "CPHL" , "Oxygen" , "Saturation" , "east" , "mean3" , "mean5" ,
                  "mean10" , "mean25" , "north" , "rugosity" , "BPI_F_10" , "BPI_F_15" , "BPI_F_25" , "BPI_B_50" ,
                  "BPI_B_100" , "BPI_B_250" , "BScatter" , "Depth" , "PSAL.log" , "TEMP.log" , "SST.log" , 
                  "curv" , "curv_plan" , "curv_prof" , "hyp3" , "hyp5" , "hyp10" ,
                  "hyp25" , "range3.sqrt" , "range5.sqrt" , "range10.sqrt" , "range25.sqrt" , "std3.sqrt" ,
                  "std5.sqrt" , "std10.sqrt" , "std25.sqrt" , "slope.sqrt" , "L1Hab" , "L2Hab" , "L3Hab" , "Rock.log" ,
                  "Sand.log" , "PebbleGravel.log")]

set.seed(20120823)
cv=sample(nrow(housing))
train=housing[cv[1:300],]
valid=housing[cv[301:400],]
test=housing[cv[401:nrow(housing)],]
 
ssto=sum((valid$HardCoral-mean(valid$HardCoral))^2)
evaluate <- function(th){ 
  num.cols=sum(th)
  if (num.cols == 0) return(0)
  fo.str="HardCoral ~"
  cum.cols=0
  for (i in 1:length(th)) {
    if (th[i]>0) {
      if (is.factor(train[,i])) {
        fo.str=paste(fo.str,colnames(train)[i],sep=" ")
      } else {
        fo.str=paste(fo.str," s(",colnames(train)[i],")",sep="")
      }
      cum.cols=cum.cols+1
      if (cum.cols) {
        fo.str=paste(fo.str,"+")
      }
    }
  }
  # colnames(train)[c(th,0)==1]
  fo=as.formula(fo.str)
  gam1 <- gam(fo,data=train)
  pred1 <- predict(gam1,valid,se=FALSE)
  sse <- sum((pred1-valid$HardCoral)^2,na.rm=TRUE)
  return(1-sse/ssto)
}
 
res <- tabuSearch(size = ncol(train)-1, iters = 20, objFunc = evaluate, listSize = 4,
                  config = rbinom(ncol(train)-1,1,.5), nRestarts = 4,verbose=TRUE)

library(reshape)
library(ggplot2); theme_set(theme_bw())
 
tabu.df=data.frame(res$configKeep)
colnames(tabu.df)=colnames(train)[1:(ncol(train)-1)]
tabu.df$Iteration=1:nrow(tabu.df)
tabu.df$RSquared=res$eUtilityKeep
tabu.df$Rank=rank(tabu.df$RSquared)
tabu.melt=melt(tabu.df,id=c("Iteration","RSquared","Rank"))
tabu.melt$RSquared=ifelse(tabu.melt$value==1,tabu.melt$RSquared,0)
tabu.melt$Rank=ifelse(tabu.melt$value==1,tabu.melt$Rank,0)
(pHeat01 <- ggplot(tabu.melt, aes(Iteration,variable)) + geom_tile(aes(fill = value)) +
  scale_fill_gradient(low = "white", high = "steelblue",guide=FALSE))
(pHeatRank <- ggplot(tabu.melt, aes(Iteration,variable)) + geom_tile(aes(fill = Rank)) +
  scale_fill_gradient(low = "white", high = "steelblue"))

```

